{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0f8a5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 192) (759572453.py, line 192)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 192\u001b[1;36m\u001b[0m\n\u001b[1;33m    boundary=frame')\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 192)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import argparse\n",
    "from flask import Flask, render_template, Response, request\n",
    "from PIL import Image\n",
    "import io\n",
    "UPLOAD_FOLDER = './UPLOAD_FOLDER'\n",
    "app = Flask(__name__)\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "def highlightFace(net, frame, conf_threshold=0.7):\n",
    "frameOpencvDnn = frame.copy()\n",
    "frameHeight = frameOpencvDnn.shape[0]\n",
    "frameWidth = frameOpencvDnn.shape[1]\n",
    "# Grab the frame dimensions and convert it to a blob.\n",
    "blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [\n",
    "104, 117, 123], True, False)\n",
    "# Pass the blob through the network and obtain the detections and predictions.\n",
    "net.setInput(blob)\n",
    "# net.forward() method detects the faces and stores the data in detections\n",
    "detections = net.forward()\n",
    "faceBoxes = []\n",
    "# This for loop is for drawing rectangle on detected face.\n",
    "for i in range(detections.shape[2]): # Looping over the detections.\n",
    "# Extract the confidence (i.e., probability) associated with the prediction.\n",
    "confidence = detections[0, 0, i, 2]\n",
    "if confidence > conf_threshold: # Compare it to the confidence threshold.\n",
    "# Compute the (x, y)-coordinates of the bounding box for the face.\n",
    "x1 = int(detections[0, 0, i, 3]*frameWidth)\n",
    "y1 = int(detections[0, 0, i, 4]*frameHeight)\n",
    "x2 = int(detections[0, 0, i, 5]*frameWidth)\n",
    "y2 = int(detections[0, 0, i, 6]*frameHeight)\n",
    "# Drawing the bounding box of the face.\n",
    "faceBoxes.append([x1, y1, x2, y2])\n",
    "cv2.rectangle(frameOpencvDnn, (x1, y1), (x2, y2),\n",
    "(0, 255, 0), int(round(frameHeight/150)), 8)\n",
    "return frameOpencvDnn, faceBoxes\n",
    "# Gives input img to the prg for detection.\n",
    "# Using argparse library which was imported.\n",
    "parser = argparse.ArgumentParser()\n",
    "# If the input argument is not given it will skip this and open webcam for detection\n",
    "parser.add_argument('--image')\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "Each model comes with two files: weight file and model file\n",
    "weight file stores the data of the deployment of the model\n",
    "model file stores actual predication done by the model\n",
    "We are using pre trained models\n",
    "The .prototxt file(s) which define the model architecture (i.e., the layers themselves)\n",
    "The .caffemodel file which contains the weights for the actual layers\n",
    "Both files are required when using models trained using Caffe for deep learning.\n",
    "'''\n",
    "def gen_frames():\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "# Defining age range.\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)',\n",
    "'(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "# LOAD NETWORK\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "# Open a video file or an image file or a camera stream\n",
    "video = cv2.VideoCapture(0)\n",
    "padding = 20\n",
    "while cv2.waitKey(1) < 0:\n",
    "# Read frame\n",
    "hasFrame, frame = video.read()\n",
    "if not hasFrame:\n",
    "cv2.waitKey()\n",
    "break\n",
    "# It will detect the no. of faces in the frame\n",
    "resultImg, faceBoxes = highlightFace(faceNet, frame)\n",
    "if not faceBoxes: # If no faces are detected\n",
    "print(\"No face detected\") # Then it will print this message\n",
    "for faceBox in faceBoxes:\n",
    "# print facebox\n",
    "face = frame[max(0, faceBox[1]-padding): # Face info is stored in this variable\n",
    "min(faceBox[3]+padding, frame.shape[0]-1), max(0,\n",
    "faceBox[0]-padding):min(faceBox[2]+padding, frame.shape[1]-1)]\n",
    "# The dnn.blobFromImage takes care of pre-processing\n",
    "# which includes setting the blob dimensions and normalization.\n",
    "blob = cv2.dnn.blobFromImage(\n",
    "face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "genderNet.setInput(blob)\n",
    "# genderNet.forward method will detect the gender of each face detected\n",
    "genderPreds = genderNet.forward()\n",
    "gender = genderList[genderPreds[0].argmax()]\n",
    "print(f'Gender: {gender}') # print the gender in the console\n",
    "ageNet.setInput(blob)\n",
    "# ageNet.forward method will detect the age of the face detected\n",
    "agePreds = ageNet.forward()\n",
    "age = ageList[agePreds[0].argmax()]\n",
    "print(f'Age: {age[1:-1]} years') # print the age in the console\n",
    "# Show the output frame\n",
    "cv2.putText(resultImg, f'{gender}, {age}', (\n",
    "faceBox[0], faceBox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "#cv2.imshow(\"Detecting age and gender\", resultImg)\n",
    "if resultImg is None:\n",
    "continue\n",
    "ret, encodedImg = cv2.imencode('.jpg', resultImg)\n",
    "#resultImg = buffer.tobytes()\n",
    "yield (b'--frame\\r\\n'\n",
    "b'Content-Type: image/jpeg\\r\\n\\r\\n' + bytearray(encodedImg) + b'\\r\\n')\n",
    "def gen_frames_photo(img_file):\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "# Defining age range.\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)',\n",
    "'(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "# LOAD NETWORK\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "# Open a video file or an image file or a camera stream\n",
    "frame = cv2.cvtColor(img_file, cv2.COLOR_BGR2RGB)\n",
    "#frame = img_file\n",
    "#hasFrame, frame = img_file.read()\n",
    "#ret, frame = cv2.imencode('.jpg', img_file)\n",
    "#video = cv2.VideoCapture(img_file)\n",
    "padding = 20\n",
    "while cv2.waitKey(1) < 0:\n",
    "# Read frame\n",
    "#hasFrame, frame = video.read()\n",
    "# if not hasFrame:\n",
    "# cv2.waitKey()\n",
    "# break\n",
    "# It will detect the no. of faces in the frame\n",
    "resultImg, faceBoxes = highlightFace(faceNet, frame)\n",
    "if not faceBoxes: # If no faces are detected\n",
    "print(\"No face detected\") # Then it will print this message\n",
    "for faceBox in faceBoxes:\n",
    "# print facebox\n",
    "face = frame[max(0, faceBox[1]-padding): # Face info is stored in this variable\n",
    "min(faceBox[3]+padding, frame.shape[0]-1), max(0,\n",
    "faceBox[0]-padding):min(faceBox[2]+padding, frame.shape[1]-1)]\n",
    "# The dnn.blobFromImage takes care of pre-processing\n",
    "# which includes setting the blob dimensions and normalization.\n",
    "blob = cv2.dnn.blobFromImage(\n",
    "face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "genderNet.setInput(blob)\n",
    "# genderNet.forward method will detect the gender of each face detected\n",
    "genderPreds = genderNet.forward()\n",
    "gender = genderList[genderPreds[0].argmax()]\n",
    "print(f'Gender: {gender}') # print the gender in the console\n",
    "ageNet.setInput(blob)\n",
    "# ageNet.forward method will detect the age of the face detected\n",
    "agePreds = ageNet.forward()\n",
    "age = ageList[agePreds[0].argmax()]\n",
    "print(f'Age: {age[1:-1]} years') # print the age in the console\n",
    "# Show the output frame\n",
    "cv2.putText(resultImg, f'{gender}, {age}', (\n",
    "faceBox[0], faceBox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "#cv2.imshow(\"Detecting age and gender\", resultImg)\n",
    "if resultImg is None:\n",
    "continue\n",
    "ret, encodedImg = cv2.imencode('.jpg', resultImg)\n",
    "#resultImg = buffer.tobytes()\n",
    "return (b'--frame\\r\\n'\n",
    "b'Content-Type: image/jpeg\\r\\n\\r\\n' + bytearray(encodedImg) + b'\\r\\n')\n",
    "@app.route('/')\n",
    "def index():\n",
    "\"\"\"Video streaming home page.\"\"\"\n",
    "return render_template('index.html')\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "# Video streaming route. Put this in the src attribute of an img tag\n",
    "return Response(gen_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "@app.route('/webcam')\n",
    "def webcam():\n",
    "return render_template('webcam.html')\n",
    "@app.route('/upload', methods=['GET', 'POST'])\n",
    "def upload_file():\n",
    "if request.method == 'POST':\n",
    "f = request.files['fileToUpload'].read()\n",
    "img = Image.open(io.BytesIO(f))\n",
    "img_ip = np.asarray(img, dtype=\"uint8\")\n",
    "print(img_ip)\n",
    "return Response(gen_frames_photo(img_ip), mimetype='multipart/x-mixed-replace';\n",
    "boundary=frame')\n",
    "# return 'file uploaded successfully'\n",
    "if __name__ == '__main__':\n",
    "app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c2b122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
